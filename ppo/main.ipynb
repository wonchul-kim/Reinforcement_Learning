{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 0 |Ep_r: -1898 \n",
      "Ep: 1 |Ep_r: -1396 \n",
      "Ep: 2 |Ep_r: -1590 \n",
      "Ep: 3 |Ep_r: -1164 \n",
      "Ep: 4 |Ep_r: -1105 \n",
      "Ep: 5 |Ep_r: -1793 \n",
      "Ep: 6 |Ep_r: -1497 \n",
      "Ep: 7 |Ep_r: -1308 \n",
      "Ep: 8 |Ep_r: -1354 \n",
      "Ep: 9 |Ep_r: -1752 \n",
      "Ep: 10 |Ep_r: -1301 \n",
      "Ep: 11 |Ep_r: -1847 \n",
      "Ep: 12 |Ep_r: -1391 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7c2b5c882585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscounted_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mbuffer_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mall_ep_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mall_ep_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_ep_r\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.9\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mep_r\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7c2b5c882585>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, s, a, r)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mMETHOD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lam'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMETHOD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lam'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# sometimes explode, this clipping is my solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# clipping method, find this is better (OpenAI's paper)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfa\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfadv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madv\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_UPDATE_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# update critic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7c2b5c882585>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mMETHOD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lam'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMETHOD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lam'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# sometimes explode, this clipping is my solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# clipping method, find this is better (OpenAI's paper)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfa\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfadv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madv\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_UPDATE_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# update critic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/p3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/p3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/p3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/p3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/p3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/p3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A simple version of Proximal Policy Optimization (PPO) using single thread.\n",
    "\n",
    "Based on:\n",
    "1. Emergence of Locomotion Behaviours in Rich Environments (Google Deepmind): [https://arxiv.org/abs/1707.02286]\n",
    "2. Proximal Policy Optimization Algorithms (OpenAI): [https://arxiv.org/abs/1707.06347]\n",
    "\n",
    "View more on my tutorial website: https://morvanzhou.github.io/tutorials\n",
    "\n",
    "Dependencies:\n",
    "tensorflow r1.2\n",
    "gym 0.9.2\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "EP_MAX = 1000\n",
    "EP_LEN = 200\n",
    "GAMMA = 0.9\n",
    "A_LR = 0.0001\n",
    "C_LR = 0.0002\n",
    "BATCH = 32\n",
    "A_UPDATE_STEPS = 10\n",
    "C_UPDATE_STEPS = 10\n",
    "S_DIM, A_DIM = 3, 1\n",
    "METHOD = [\n",
    "    dict(name='kl_pen', kl_target=0.01, lam=0.5),   # KL penalty\n",
    "    dict(name='clip', epsilon=0.2),                 # Clipped surrogate objective, find this is better\n",
    "][1]        # choose the method for optimization\n",
    "\n",
    "\n",
    "class PPO(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.sess = tf.Session()\n",
    "        self.tfs = tf.placeholder(tf.float32, [None, S_DIM], 'state')\n",
    "\n",
    "        # critic\n",
    "        with tf.variable_scope('critic'):\n",
    "            l1 = tf.layers.dense(self.tfs, 100, tf.nn.relu)\n",
    "            self.v = tf.layers.dense(l1, 1)\n",
    "            self.tfdc_r = tf.placeholder(tf.float32, [None, 1], 'discounted_r')\n",
    "            self.advantage = self.tfdc_r - self.v\n",
    "            self.closs = tf.reduce_mean(tf.square(self.advantage))\n",
    "            self.ctrain_op = tf.train.AdamOptimizer(C_LR).minimize(self.closs)\n",
    "\n",
    "        # actor\n",
    "        pi, pi_params = self._build_anet('pi', trainable=True)\n",
    "        oldpi, oldpi_params = self._build_anet('oldpi', trainable=False)\n",
    "        with tf.variable_scope('sample_action'):\n",
    "            self.sample_op = tf.squeeze(pi.sample(1), axis=0)       # choosing action\n",
    "        with tf.variable_scope('update_oldpi'):\n",
    "            self.update_oldpi_op = [oldp.assign(p) for p, oldp in zip(pi_params, oldpi_params)]\n",
    "\n",
    "        self.tfa = tf.placeholder(tf.float32, [None, A_DIM], 'action')\n",
    "        self.tfadv = tf.placeholder(tf.float32, [None, 1], 'advantage')\n",
    "        with tf.variable_scope('loss'):\n",
    "            with tf.variable_scope('surrogate'):\n",
    "                # ratio = tf.exp(pi.log_prob(self.tfa) - oldpi.log_prob(self.tfa))\n",
    "                ratio = pi.prob(self.tfa) / oldpi.prob(self.tfa)\n",
    "                surr = ratio * self.tfadv\n",
    "            if METHOD['name'] == 'kl_pen':\n",
    "                self.tflam = tf.placeholder(tf.float32, None, 'lambda')\n",
    "                kl = tf.distributions.kl_divergence(oldpi, pi)\n",
    "                self.kl_mean = tf.reduce_mean(kl)\n",
    "                self.aloss = -(tf.reduce_mean(surr - self.tflam * kl))\n",
    "            else:   # clipping method, find this is better\n",
    "                self.aloss = -tf.reduce_mean(tf.minimum(\n",
    "                    surr,\n",
    "                    tf.clip_by_value(ratio, 1.-METHOD['epsilon'], 1.+METHOD['epsilon'])*self.tfadv))\n",
    "\n",
    "        with tf.variable_scope('atrain'):\n",
    "            self.atrain_op = tf.train.AdamOptimizer(A_LR).minimize(self.aloss)\n",
    "\n",
    "        tf.summary.FileWriter(\"log/\", self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def update(self, s, a, r):\n",
    "        self.sess.run(self.update_oldpi_op)\n",
    "        adv = self.sess.run(self.advantage, {self.tfs: s, self.tfdc_r: r})\n",
    "        # adv = (adv - adv.mean())/(adv.std()+1e-6)     # sometimes helpful\n",
    "\n",
    "        # update actor\n",
    "        if METHOD['name'] == 'kl_pen':\n",
    "            for _ in range(A_UPDATE_STEPS):\n",
    "                _, kl = self.sess.run(\n",
    "                    [self.atrain_op, self.kl_mean],\n",
    "                    {self.tfs: s, self.tfa: a, self.tfadv: adv, self.tflam: METHOD['lam']})\n",
    "                if kl > 4*METHOD['kl_target']:  # this in in google's paper\n",
    "                    break\n",
    "            if kl < METHOD['kl_target'] / 1.5:  # adaptive lambda, this is in OpenAI's paper\n",
    "                METHOD['lam'] /= 2\n",
    "            elif kl > METHOD['kl_target'] * 1.5:\n",
    "                METHOD['lam'] *= 2\n",
    "            METHOD['lam'] = np.clip(METHOD['lam'], 1e-4, 10)    # sometimes explode, this clipping is my solution\n",
    "        else:   # clipping method, find this is better (OpenAI's paper)\n",
    "            [self.sess.run(self.atrain_op, {self.tfs: s, self.tfa: a, self.tfadv: adv}) for _ in range(A_UPDATE_STEPS)]\n",
    "\n",
    "        # update critic\n",
    "        [self.sess.run(self.ctrain_op, {self.tfs: s, self.tfdc_r: r}) for _ in range(C_UPDATE_STEPS)]\n",
    "\n",
    "    def _build_anet(self, name, trainable):\n",
    "        with tf.variable_scope(name):\n",
    "            l1 = tf.layers.dense(self.tfs, 100, tf.nn.relu, trainable=trainable)\n",
    "            mu = 2 * tf.layers.dense(l1, A_DIM, tf.nn.tanh, trainable=trainable)\n",
    "            sigma = tf.layers.dense(l1, A_DIM, tf.nn.softplus, trainable=trainable)\n",
    "            norm_dist = tf.distributions.Normal(loc=mu, scale=sigma)\n",
    "        params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)\n",
    "        return norm_dist, params\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]\n",
    "        a = self.sess.run(self.sample_op, {self.tfs: s})[0]\n",
    "        return np.clip(a, -2, 2)\n",
    "\n",
    "    def get_v(self, s):\n",
    "        if s.ndim < 2: s = s[np.newaxis, :]\n",
    "        return self.sess.run(self.v, {self.tfs: s})[0, 0]\n",
    "\n",
    "env = gym.make('Pendulum-v0').unwrapped\n",
    "ppo = PPO()\n",
    "all_ep_r = []\n",
    "\n",
    "for ep in range(EP_MAX):\n",
    "    s = env.reset()\n",
    "    buffer_s, buffer_a, buffer_r = [], [], []\n",
    "    ep_r = 0\n",
    "    for t in range(EP_LEN):    # in one episode\n",
    "#         env.render()\n",
    "        a = ppo.choose_action(s)\n",
    "        s_, r, done, _ = env.step(a)\n",
    "        buffer_s.append(s)\n",
    "        buffer_a.append(a)\n",
    "        buffer_r.append((r+8)/8)    # normalize reward, find to be useful\n",
    "        s = s_\n",
    "        ep_r += r\n",
    "\n",
    "        # update ppo\n",
    "        if (t+1) % BATCH == 0 or t == EP_LEN-1:\n",
    "            v_s_ = ppo.get_v(s_)\n",
    "            discounted_r = []\n",
    "            for r in buffer_r[::-1]:\n",
    "                v_s_ = r + GAMMA * v_s_\n",
    "                discounted_r.append(v_s_)\n",
    "            discounted_r.reverse()\n",
    "\n",
    "            bs, ba, br = np.vstack(buffer_s), np.vstack(buffer_a), np.array(discounted_r)[:, np.newaxis]\n",
    "            buffer_s, buffer_a, buffer_r = [], [], []\n",
    "            ppo.update(bs, ba, br)\n",
    "    if ep == 0: all_ep_r.append(ep_r)\n",
    "    else: all_ep_r.append(all_ep_r[-1]*0.9 + ep_r*0.1)\n",
    "    print(\n",
    "        'Ep: %i' % ep,\n",
    "        \"|Ep_r: %i\" % ep_r,\n",
    "        (\"|Lam: %.4f\" % METHOD['lam']) if METHOD['name'] == 'kl_pen' else '',\n",
    "    )\n",
    "\n",
    "plt.plot(np.arange(len(all_ep_r)), all_ep_r)\n",
    "plt.xlabel('Episode');plt.ylabel('Moving averaged episode reward');plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
