{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* contributions of DQN\n",
    "> 1. understand image itself as states(input)\n",
    "  2. can make better performance than human\n",
    "  3. with replay memory, solve the problem of the relation b/t samples\n",
    "\n",
    "* limits of DQN\n",
    "> 1. requires huge memory\n",
    "  2. low training speed\n",
    "  3. unstable policy training b/c it is value based\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C (Asynchronous Advantage Actor-Critic)\n",
    "1. solve the problem of the relation b/t samples with asynchronous updates\n",
    "2. does not use replay memory\n",
    "3. can apply policy gradient algorithm (actor-critic)\n",
    "4. relatively high training speed b/c it uses many agents in each envir.<br />\n",
    "\n",
    "\n",
    ">A3C = Asynchronous update + Actor-Critic\n",
    ">> Actor-Critic = REINFORCE + real-time training\n",
    ">>> REINFORCE = Monte Carlo policy gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### cf) In supervised learning,\n",
    "<img src=\"./figs/supervise1.png\">\n",
    "\n",
    "> $\\sum y_i logp_i$ represents the difference b/t predcition(p) and answer(y)\n",
    "and then, gradient descent update the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In reinforcement learning, \n",
    "<img src=\"./figs/pg1.png\">\n",
    "\n",
    "> $\\sum y_i logp_i$ represents the difference b/t predcition(p) and answer(y)\n",
    "\n",
    "<img src=\"./figs/pg2.png\">\n",
    "\n",
    "> Need to think the meaning of making the predcition(p) to the action taken(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcuating $Q(s_t, a_t)$, how good the action($a_t$) taken on the current state($s_t$) is, the process of the above teach the agent how to act.<br /> \n",
    "($Q_\\pi(s, a) = E_\\pi[R{t+1} + \\gamma R{t+2} + ... | s_t=s, a_t=a]$) \n",
    "<img src=\"./figs/pg3.png\"><br />\n",
    "Specifically, the network is trained to predict the action which makes the agent to do the action providing better rewards. By training according to the gradient descent of the difference b/t prediction(p) and answer(y) multiplied by Q value function, the agent will be aware of which action is  good for better rewards. In other words, policy gradient in RL increase the probability of the action providing better rewards.\n",
    "\n",
    "To sum up, within policy gradient, there are two important factors:\n",
    "* the standard to update the policy: objective function\n",
    "* the method to update the policy with the objective function: gradient ascent\n",
    "> Policy: $\\pi_\\theta (a|s) = P[a_t=a|s_t=s, \\theta]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective func. , $J(\\theta)$\n",
    ": produce the information about which direction for the policy to be updated     \n",
    "\n",
    "> $ J(\\theta) = E[\\sum^{T-1}_{t=0} r_{t+1}|\\pi_{\\theta}]$ \n",
    "              = $E[r_1 + r_2 + r_3 + ... + r_T|\\pi_\\theta]$\n",
    ">>  cumulated expected rewards according to a trajectory\n",
    "\n",
    "\n",
    "With the above objective func., <br>\n",
    "the $\\theta$ can be updated by the bellow: <br>\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">$\\theta' = \\theta + \\alpha \\nabla_\\theta J(\\theta)$</div>\n",
    "<div align=\"center\">($\\nabla_\\theta J(\\theta)$: policy gradient)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In REINFORCE algorithm,<br>\n",
    "<div align=\"center\">$\\nabla_\\theta J(\\theta) ~ \\sum^{T-1}_{t=0} \\nabla_\\theta log\\pi_\\theta(a_t|s_t)G_t$</div><br>\n",
    "\n",
    "\n",
    "$J(\\theta) = E[\\sum^{T-1}_{t=0}r_{t+1}|\\pi_\\theta]$<br><br>\n",
    "$ \\ \\ \\ \\ \\ \\ \\  = E[r_1|\\pi_\\theta] + E[r_2|\\pi_\\theta] + ... + E[r_T|\\pi_\\theta]$<br><br>\n",
    "According to the fact that expected value is probability of ($x$) * output of ($x$),\n",
    "\n",
    "<img src=\"./figs/REINFORCE1.png\"> <br>\n",
    "Differenciating the both sides, \n",
    "<img src=\"./figs/REINFORCE2.png\" width=\"400\"><br>\n",
    "The final step can be done by the bellow:\n",
    "<img src=\"./figs/REINFORCE3.png\" width=\"500\"><br>\n",
    "\n",
    "To sum up, \n",
    "<img src=\"./figs/REINFORCE4.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the total sum of any probability is 1 and log(1) = 0, \n",
    "<img src=\"./figs/REINFORCE5.png\" width=\"500\">\n",
    "<img src=\"./figs/REINFORCE6.png\" width=\"500\"><br>\n",
    "Differenciating the both sides, \n",
    "<img src=\"./figs/REINFORCE7.png\" width=\"500\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "_________________\n",
    "Finally, \n",
    "<img src=\"./figs/REINFORCE8.png\" > <br>\n",
    "\n",
    "With a discount factor,\n",
    "<img src=\"./figs/REINFORCE9.png\" > <br>\n",
    "<img src=\"./figs/REINFORCE10.png\" > <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pseudo code for REINFORCE algorithm\n",
    "1. execute the policy within one episode and record trajectory\n",
    "2. according to one episode/trajectory, calculate $G_t$\n",
    "3. update policy based on the policy gradient <br>\n",
    "$ \\ \\ \\  ( policy gradient = \\sum^{T-1}_{t=0}\\nabla_\\theta log\\pi_\\theta(a_t|s_t)G_t )$\n",
    "4. go to 1\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "> update episode by episode = MCT policy gradient = REINFORCE\n",
    "\n",
    "\n",
    "<U>However, there are some limits of REINFORCE</U>:\n",
    "1. update episode by spisode (not online)\n",
    "2. high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actor-Critic method with advantage function\n",
    "\n",
    "To overcome those limits,\n",
    "1. Apply actor-critic method on REINFORCE\n",
    "\n",
    "<img src=\"./figs/AC1.png\" width=\"500\">\n",
    "<img src=\"./figs/AC2.png\" width=\"500\"><br>\n",
    "If we know $Q_{\\pi_{\\theta}} (s_t, a_t)$, we can update the net. step by step. Thus, need to approximate it:<br><br>\n",
    "<div align=\"center\"> $Q_{\\pi_{\\theta}} (s_t, a_t) \\ $~$ \\ Q_w(s_t, a_t)$</div>\n",
    "\n",
    "> Critic will do this!!!    \n",
    ">> Accordingly, <br><br>\n",
    ">> $\\nabla_\\theta J(\\theta) \\ \\ $~$ \\ \\ \\nabla_\\theta log\\pi_\\theta (a_t|s_t)Q_w(s_t, a_t)$\n",
    "\n",
    "\n",
    "2. apply advantage function(Q value function - baseline value function)\n",
    "<br>\n",
    "<div align=\"center\">$\\nabla_\\theta J(\\theta) \\ \\ $~$ \\ \\ \\nabla_\\theta log\\pi_\\theta (a_t|s_t)(Q_w(s_t, a_t) - V_v(s_t))$\n",
    "\n",
    "\n",
    "In this process, it costs too much to approximate both of the Q value function and the value function. Thus, we can use \n",
    "<div align=\"center\">$E[r_{t+1} + \\gamma V(s_{t+1}) | s_t, a_t]$</div>\n",
    "<br>\n",
    "Then, <br>\n",
    "<div align=\"center\">$\\nabla_\\theta J(\\theta) \\ \\ $~$ \\ \\ \\nabla_\\theta log\\pi_\\theta (a_t|s_t)(r_{t+1} + \\gamma V_v(s_{t+1}) - V_v(s_t))$</div>\n",
    "\n",
    "_________________\n",
    "> Actor: \n",
    ">>        1) approximate $\\theta$\n",
    ">>        2) update with \n",
    "<div align=\"center\">$\\nabla_\\theta J(\\theta) \\ \\ $~$ \\ \\ \\nabla_\\theta \n",
    "log\\pi_\\theta (a_t|s_t)(r_{t+1} + \\gamma V_v(s_{t+1}) - V_v(s_t))$</div>\n",
    "\n",
    "\n",
    "> Critic: \n",
    ">>        1) approximate value function\n",
    ">>        2) update with the error,\n",
    "<div align=\"center\">$(r_{t+1} + \\gamma V_v(s_{t+1}) - V_v(s_t))^2$</div>\n",
    "\n",
    "<br>\n",
    "<img src=\"./figs/AC3.png\">\n",
    "<br>\n",
    "<img src=\"./figs/AC4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Asynchronous update, \n",
    "Each agent has its own independent environment and they are called local networks.\n",
    "<img src=\"./figs/a3c1.png\"><br >\n",
    "then, the global network is updated by and updates those each local networks each other <U>asynchronously</U>.  \n",
    "_____________\n",
    "1. Multi-step loss function <br>\n",
    "<img src=\"./figs/a3c2.png\"><br >\n",
    "<img src=\"./figs/a3c3.png\"><br >\n",
    "<br><br>\n",
    "\n",
    "2. Entropy loss function <br>\n",
    "<img src=\"./figs/a3c4.png\"><br >\n",
    "<img src=\"./figs/a3c5.png\"><br >\n",
    "<img src=\"./figs/a3c6.png\"><br >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
