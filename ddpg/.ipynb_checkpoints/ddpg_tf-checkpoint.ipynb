{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import argparse\n",
    "import pprint as pp\n",
    "import tflearn\n",
    "\n",
    "from replay_buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state, output is the action\n",
    "    under a deterministic policy.\n",
    "    The output layer activation is a tanh to keep the action\n",
    "    between -action_bound and action_bound\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, state_dim, action_dim, action_bound, learning_rate, tau, batch_size):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Actor Network\n",
    "        with tf.variable_scope('actor'):\n",
    "            self.inputs, self.out, self.scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()\n",
    "\n",
    "        # Target Network\n",
    "        with tf.variable_scope('target'):\n",
    "            self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.target_network_params = tf.trainable_variables()[\n",
    "            len(self.network_params):]\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) +\n",
    "                                                  tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # This gradient will be provided by the critic network\n",
    "        self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])\n",
    "\n",
    "        # Combine the gradients here\n",
    "        self.unnormalized_actor_gradients = tf.gradients(\n",
    "            self.scaled_out, self.network_params, -self.action_gradient)\n",
    "        self.actor_gradients = list(map(lambda x: tf.div(x, self.batch_size), self.unnormalized_actor_gradients))\n",
    "\n",
    "        # Optimization Op\n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).\\\n",
    "            apply_gradients(zip(self.actor_gradients, self.network_params))\n",
    "\n",
    "        self.num_trainable_vars = len(\n",
    "            self.network_params) + len(self.target_network_params)\n",
    "\n",
    "    def create_actor_network(self):   \n",
    "        inputs = tf.placeholder(tf.float32, [None, self.s_dim])\n",
    "        W1 = tf.get_variable(\"W1\", [self.s_dim, 16],\n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "#         b1 = tf.get_variable(\"b1\", [16],\n",
    "#                         initializer=tf.constant_initializer(0))\n",
    "        b1 = tf.Variable(tf.random_normal([16]))\n",
    "        L1 = tf.nn.relu(tf.add(tf.matmul(inputs, W1), b1))\n",
    "#         L1 = tf.contrib.layers.batch_norm(tf.add(tf.matmul(inputs, W1), b1))\n",
    "#         L1 = tf.nn.relu(L1)\n",
    "\n",
    "        W2 = tf.get_variable(\"W2\", [16, 64],\n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "#         b2 = tf.get_variable(\"b2\", [64],\n",
    "#                         initializer=tf.constant_initializer(0))\n",
    "        b2 = tf.Variable(tf.random_normal([64]))\n",
    "        L2 = tf.nn.relu(tf.add(tf.matmul(L1, W2), b2))\n",
    "#         L2 = tf.contrib.layers.batch_norm(tf.add(tf.matmul(L1, W2), b2))\n",
    "#         L2 = tf.nn.relu(L2)\n",
    "\n",
    "        \n",
    "        W3 = tf.get_variable(\"W3\", [64, self.a_dim],\n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "#         b3 = tf.get_variable(\"b3\", [self.a_dim],\n",
    "#                         initializer=tf.constant_initializer(0))\n",
    "        b3 = tf.Variable(tf.random_normal([self.a_dim]))\n",
    "\n",
    "        out = tf.nn.tanh(tf.add(tf.matmul(L2, W3), b3))\n",
    "        x = tf.layers.dense(inputs, units=64, activation=tf.nn.relu, name='p_fc0')\n",
    "        x = tf.layers.dense(x, units=64, activation=tf.nn.relu, name='p_fc1')\n",
    "\n",
    "#         for i in range(2):\n",
    "#             x = tf.layers.dense(inputs, units=64, activation=tf.nn.relu, name='p_fc'+str(i))\n",
    "        \n",
    "#         out = tf.layers.dense(x, units=self.a_dim, activation=tf.nn.tanh, name='p_fc2')\n",
    "        scaled_out = tf.multiply(out, self.action_bound)\n",
    "        return inputs, out, scaled_out\n",
    "\n",
    "    def train(self, inputs, a_gradient):\n",
    "        self.sess.run(self.optimize, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action_gradient: a_gradient\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.sess.run(self.scaled_out, feed_dict={\n",
    "            self.inputs: inputs\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs):\n",
    "        return self.sess.run(self.target_scaled_out, feed_dict={\n",
    "            self.target_inputs: inputs\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "\n",
    "    def get_num_trainable_vars(self):\n",
    "        return self.num_trainable_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state and action, output is Q(s,a).\n",
    "    The action must be obtained from the output of the Actor network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, gamma, num_actor_vars):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Create the critic network\n",
    "        with tf.variable_scope('critic'):\n",
    "            self.inputs, self.action, self.out = self.create_critic_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()[num_actor_vars:]\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_action, self.target_out = self.create_critic_network()\n",
    "\n",
    "        self.target_network_params = tf.trainable_variables()[(len(self.network_params) + num_actor_vars):]\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights with regularization\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) \\\n",
    "            + tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # Network target (y_i)\n",
    "        self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        # Define loss and optimization Op\n",
    "        self.loss = tflearn.mean_square(self.predicted_q_value, self.out)\n",
    "        self.optimize = tf.train.AdamOptimizer(\n",
    "            self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        # Get the gradient of the net w.r.t. the action.\n",
    "        # For each action in the minibatch (i.e., for each x in xs),\n",
    "        # this will sum up the gradients of each critic output in the minibatch\n",
    "        # w.r.t. that action. Each output is independent of all\n",
    "        # actions except for one.\n",
    "        self.action_grads = tf.gradients(self.out, self.action)\n",
    "\n",
    "    def create_critic_network(self):\n",
    "        inputs = tf.placeholder(tf.float32, [None, self.s_dim])\n",
    "        action = tf.placeholder(tf.float32, [None, self.a_dim])\n",
    "        \n",
    "#         l1 = tflearn.fully_connected(inputs, 16)\n",
    "# #         l1 = tflearn.layers.normalization.batch_normalization(l1)\n",
    "#         l1 = tflearn.activations.relu(l1)\n",
    "        \n",
    "        w1 = tf.get_variable(\"w1\", [self.s_dim, 16], \\\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b1 = tf.Variable(tf.random_normal([16]), 'b1')\n",
    "        l1 = tf.nn.relu(tf.add(tf.matmul(inputs, w1), b1))\n",
    "        \n",
    "        # Add the action tensor in the 2nd hidden layer\n",
    "        # Use two temp layers to get the corresponding weights and biases\n",
    "        w2 = tf.get_variable('w2', [16, 64], \\\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b2 = tf.Variable(tf.random_normal([64]))\n",
    "        \n",
    "        w2_ = tf.get_variable('w2_', [self.a_dim, 64], \\\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b2_ = tf.Variable(tf.random_normal([64]))\n",
    "        \n",
    "#         net = tflearn.activation(\n",
    "#             tf.matmul(l1, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')\n",
    "\n",
    "        l2 = tf.nn.relu(\n",
    "            tf.matmul(l1, w2) + b2 + tf.matmul(action, w2_) + b2_)\n",
    "\n",
    "        # linear layer connected to 1 output representing Q(s,a)\n",
    "        # Weights are init to Uniform[-3e-3, 3e-3]\n",
    "#         w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "#         out = tflearn.fully_connected(net, 1, weights_init=w_init)\n",
    "        w3 = tf.get_variable('w3', [64, 1], \\\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b3 = tf.Variable(tf.random_normal([1]))\n",
    "        out = tf.matmul(l2, w3) + b3\n",
    "        return inputs, action, out\n",
    "\n",
    "    def train(self, inputs, action, predicted_q_value):\n",
    "        return self.sess.run([self.out, self.optimize], feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action,\n",
    "            self.predicted_q_value: predicted_q_value\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs, action):\n",
    "        return self.sess.run(self.out, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs, action):\n",
    "        return self.sess.run(self.target_out, feed_dict={\n",
    "            self.target_inputs: inputs,\n",
    "            self.target_action: action\n",
    "        })\n",
    "\n",
    "    def action_gradients(self, inputs, actions):\n",
    "        return self.sess.run(self.action_grads, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: actions\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma=0.3, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_summaries():\n",
    "    episode_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Reward\", episode_reward)\n",
    "    episode_ave_max_q = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Qmax Value\", episode_ave_max_q)\n",
    "\n",
    "    summary_vars = [episode_reward, episode_ave_max_q]\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "\n",
    "    return summary_ops, summary_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = '1234'\n",
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bound = env.action_space.high\n",
    "assert (env.action_space.high == -env.action_space.low)\n",
    "\n",
    "actor_lr = 1e-3\n",
    "critic_lr = 1e-2\n",
    "tau = 1e-3\n",
    "minibatch_size = 64\n",
    "gamma = 0.99\n",
    "buffer_size = 1000000\n",
    "\n",
    "max_episodes = 200\n",
    "max_episodes_len = 1000\n",
    "render_env = False\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session() \n",
    "\n",
    "actor = ActorNetwork(sess, state_dim, action_dim, action_bound,\n",
    "                     actor_lr, tau, minibatch_size)\n",
    "\n",
    "critic = CriticNetwork(sess, state_dim, action_dim,\n",
    "                       critic_lr, tau, gamma, actor.get_num_trainable_vars())\n",
    "\n",
    "actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -1111 | Episode: 0 | Qmax: -1.8065\n",
      "\n",
      "Reward: -1614 | Episode: 1 | Qmax: -0.6349\n",
      "\n",
      "Reward: -1585 | Episode: 2 | Qmax: -0.7878\n",
      "\n",
      "Reward: -838 | Episode: 3 | Qmax: -0.7609"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-fb2823b41816>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0ma_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m# Update target networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-c006833bf1ad>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, inputs, a_gradient)\u001b[0m\n\u001b[1;32m     90\u001b[0m         self.sess.run(self.optimize, feed_dict={\n\u001b[1;32m     91\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_gradient\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma_gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         })\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/p3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/p3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1095\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/p3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mdirect\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \"\"\"\n\u001b[0;32m--> 428\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/p3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_controller\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m   5221\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_controller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5222\u001b[0m     context.context().context_switches.push(\n\u001b[0;32m-> 5223\u001b[0;31m         default.building_function, default.as_default)\n\u001b[0m\u001b[1;32m   5224\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5225\u001b[0m       with super(_DefaultGraphStack, self).get_controller(\n",
      "\u001b[0;32m~/.virtualenvs/p3/lib/python3.6/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mpush\u001b[0;34m(self, is_building_function, enter_context_fn)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     self.stack.append(\n\u001b[0;32m--> 136\u001b[0;31m         ContextSwitch(is_building_function, enter_context_fn))\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(_cls, is_building_function, enter_context_fn)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# if args['use_gym_monitor']:\n",
    "#     if not args['render_env']:\n",
    "#         env = wrappers.Monitor(\n",
    "#             env, args['monitor_dir'], video_callable=False, force=True)\n",
    "#     else:\n",
    "#         env = wrappers.Monitor(env, args['monitor_dir'], force=True)\n",
    "\n",
    "\n",
    "# if args['use_gym_monitor']:\n",
    "#     env.monitor.close()\n",
    "\n",
    "\n",
    "\n",
    "# summary_ops, summary_vars = build_summaries()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# writer = tf.summary.FileWriter(args['summary_dir'], sess.graph)\n",
    "\n",
    "# Initialize target network weights\n",
    "actor.update_target_network()\n",
    "critic.update_target_network()\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "\n",
    "R, Qmax = [], []\n",
    "for i in range(max_episodes):\n",
    "\n",
    "    s = env.reset()\n",
    "\n",
    "    ep_reward = 0\n",
    "    ep_ave_max_q = 0\n",
    "\n",
    "    for j in range(max_episodes_len):\n",
    "\n",
    "#         if args['render_env']:\n",
    "#             env.render()\n",
    "\n",
    "        # Added exploration noise\n",
    "        #a = actor.predict(np.reshape(s, (1, 3))) + (1. / (1. + i))\n",
    "        a = actor.predict(np.reshape(s, (1, actor.s_dim))) + actor_noise()\n",
    "\n",
    "        s2, r, terminal, info = env.step(a[0])\n",
    "\n",
    "        replay_buffer.add(np.reshape(s, (actor.s_dim,)), np.reshape(a, (actor.a_dim,)), r,\n",
    "                          terminal, np.reshape(s2, (actor.s_dim,)))\n",
    "\n",
    "        # Keep adding experience to the memory until\n",
    "        # there are at least minibatch size samples\n",
    "        if replay_buffer.size() > minibatch_size:\n",
    "            s_batch, a_batch, r_batch, t_batch, s2_batch = \\\n",
    "                replay_buffer.sample_batch(minibatch_size)\n",
    "\n",
    "            # Calculate targets\n",
    "            target_q = critic.predict_target(\n",
    "                s2_batch, actor.predict_target(s2_batch))\n",
    "\n",
    "#             y_i = []\n",
    "#             for k in range(minibatch_size):\n",
    "#                 if t_batch[k]:\n",
    "#                     y_i.append(r_batch[k])\n",
    "#                 else:\n",
    "#                     y_i.append(r_batch[k] + critic.gamma * target_q[k])\n",
    "\n",
    "            for k in range(minibatch_size):\n",
    "                if not t_batch[k]:\n",
    "                    r_batch[k] = r_batch[k] + critic.gamma * target_q[k]\n",
    "\n",
    "\n",
    "            # Update the critic given the targets\n",
    "            predicted_q_value, _ = critic.train(\n",
    "                s_batch, a_batch, np.reshape(r_batch, (minibatch_size, 1)))\n",
    "\n",
    "            ep_ave_max_q += np.amax(predicted_q_value)\n",
    "\n",
    "            # Update the actor policy using the sampled gradient\n",
    "            a_outs = actor.predict(s_batch)\n",
    "            grads = critic.action_gradients(s_batch, a_outs)\n",
    "            actor.train(s_batch, grads[0])\n",
    "\n",
    "            # Update target networks\n",
    "            actor.update_target_network()\n",
    "            critic.update_target_network()\n",
    "\n",
    "        s = s2\n",
    "        ep_reward += r\n",
    "\n",
    "        print('\\rReward: {:d} | Episode: {:d} | Qmax: {:.4f}'.format(int(ep_reward), \\\n",
    "                    i, (ep_ave_max_q / (float(j)+0.1))), end='')\n",
    "        if terminal:\n",
    "            print('\\n')\n",
    "            Qmax.append(ep_ave_max_q/float(j))\n",
    "            R.append(ep_reward)\n",
    "#             summary_str = sess.run(summary_ops, feed_dict={\n",
    "#                 summary_vars[0]: ep_reward,\n",
    "#                 summary_vars[1]: ep_ave_max_q / float(j)\n",
    "#             })\n",
    "\n",
    "#             writer.add_summary(summary_str, i)\n",
    "#             writer.flush()\n",
    "\n",
    "            break\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "        \n",
    "fig = plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.plot(range(len(R)), R)\n",
    "plt.title(\"reward per episode\")\n",
    "plt.subplot(212)\n",
    "plt.plot(range(len(Qmax)), Qmax)\n",
    "plt.title(\"Qmax per episode\")\n",
    "plt.show()\n",
    "plt.savefig(\"./figs/results.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
